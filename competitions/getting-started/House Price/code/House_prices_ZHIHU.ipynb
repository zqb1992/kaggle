{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考地址\n",
    "[参考地址：Kaggle竞赛 —— 房价预测 (House Prices)](https://zhuanlan.zhihu.com/p/34904202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "__1. Exploratory Visualization__  \n",
    "__2. Data Cleaning__  \n",
    "__3. Feature Engineering__  \n",
    "__4. Modeling & Evaluation__  \n",
    "__5. Ensemble Methods__  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入基本库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#忽略一些警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#导入机器学习库\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from scipy.stats import skew\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')  #样式美化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',200)         #列宽度\n",
    "pd.set_option('display.width',200)        #横向最多显示多少个字符\n",
    "pd.set_option('display.max_columns',500)  #最大显示列数\n",
    "pd.set_option('display.max_rows',1000)    #最大显示行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('../data/train.csv')\n",
    "test=pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.info()\n",
    "print()\n",
    "print(\"*\"*30)\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __It seems that the price of recent-built houses are higher. So later I 'll use labelencoder for three \"Year\" feature.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#对年份进行可视化\n",
    "plt.figure(figsize=(15,8))\n",
    "# sns.boxplot(train.YearBuilt, train.SalePrice)\n",
    "sns.boxplot(x=\"YearBuilt\", y=\"SalePrice\", data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __As is discussed in other kernels, the bottom right two two points with extremely large GrLivArea are likely to be outliers. So we delete them.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(x=train.GrLivArea, y=train.SalePrice)\n",
    "plt.xlabel(\"GrLivArea\", fontsize=13)\n",
    "plt.ylabel(\"SalePrice\", fontsize=13)\n",
    "plt.ylim(0,800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去除GrLiveArea的离群点\n",
    "train.drop(train[(train[\"GrLivArea\"]>4000)&(train[\"SalePrice\"]<300000)].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full=pd.concat([train,test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full.drop(['Id'],axis=1, inplace=True)\n",
    "full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#查看缺失值，并排序\n",
    "aa = full.isnull().sum()\n",
    "aa[aa>0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Let's first imput the missing values of LotFrontage based on the median of LotArea and Neighborhood. Since LotArea is a continuous feature, We use qcut to divide it into 10 parts.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#agg函数对分组后的数据进行聚合处理\n",
    "full.groupby(['Neighborhood'])[['LotFrontage']].agg(['mean','median','count'])\n",
    "# print(full.groupby(['Neighborhood'])[['LotFrontage']].agg(['mean','median','count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full[\"LotAreaCut\"] = pd.qcut(full.LotArea,10) #qcut是根据这些值的频率来选择箱子的均匀间隔，即每个箱子中含有的数的数量是相同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#agg函数对分组后的数据进行聚合处理\n",
    "full.groupby(['LotAreaCut'])[['LotFrontage']].agg(['mean','median','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#中位数填充\n",
    "full['LotFrontage']=full.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#中位数填充\n",
    "# Since some combinations of LotArea and Neighborhood are not available, so we just LotAreaCut alone.\n",
    "full['LotFrontage']=full.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Then we filling in other missing values according to data_description.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对一下特征使用0来填补缺失值\n",
    "cols=[\"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"GarageCars\", \"BsmtFinSF2\", \"BsmtFinSF1\", \"GarageArea\"]\n",
    "for col in cols:\n",
    "    full[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对一下特征使用None来填补缺失值\n",
    "cols1 = [\"PoolQC\" , \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"GarageFinish\", \"GarageYrBlt\", \"GarageType\", \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\", \"MasVnrType\"]\n",
    "for col in cols1:\n",
    "    full[col].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#以下特征使用众数填充\n",
    "cols2 = [\"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\", \"Functional\", \"Electrical\", \"KitchenQual\", \"SaleType\",\"Exterior1st\", \"Exterior2nd\"]\n",
    "for col in cols2:\n",
    "    full[col].fillna(full[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __And there is no missing data except for the value we want to predict !__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#查看一下是否全部处理完\n",
    "full.isnull().sum()[full.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Convert some numerical features into categorical features. It's better to use LabelEncoder and get_dummies for these features.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#类型转化\n",
    "NumStr = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"MoSold\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"LowQualFinSF\",\"GarageYrBlt\"]\n",
    "for col in NumStr:\n",
    "    full[col]=full[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Now I want to do a long list of value-mapping. __\n",
    "+ __I was influenced by the insight that we should build as many features as possible and trust the model to choose the right features. So I decided to groupby SalePrice according to one feature and sort it based on mean and median. Here is an example:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full.groupby(['MSSubClass'])[['SalePrice']].agg(['mean','median','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __So basically I'll do__  \n",
    "                '180' : 1\n",
    "                '30' : 2   '45' : 2\n",
    "                '190' : 3, '50' : 3, '90' : 3,\n",
    "                '85' : 4, '40' : 4, '160' : 4\n",
    "                '70' : 5, '20' : 5, '75' : 5, '80' : 5, '150' : 5\n",
    "                '120': 6, '60' : 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Different people may have different views on how to map these values, so just follow your instinct =^_^=__  \n",
    "__Below I also add a small \"o\" in front of the features so as to keep the original features to use get_dummies in a moment.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_values():\n",
    "    full[\"oMSSubClass\"] = full.MSSubClass.map({'180':1, \n",
    "                                        '30':2, '45':2, \n",
    "                                        '190':3, '50':3, '90':3, \n",
    "                                        '85':4, '40':4, '160':4, \n",
    "                                        '70':5, '20':5, '75':5, '80':5, '150':5,\n",
    "                                        '120': 6, '60':6})\n",
    "    \n",
    "    full[\"oMSZoning\"] = full.MSZoning.map({'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4})\n",
    "    \n",
    "    full[\"oNeighborhood\"] = full.Neighborhood.map({'MeadowV':1,\n",
    "                                               'IDOTRR':2, 'BrDale':2,\n",
    "                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,\n",
    "                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n",
    "                                               'NPkVill':5, 'Mitchel':5,\n",
    "                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,\n",
    "                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n",
    "                                               'Veenker':8, 'Somerst':8, 'Timber':8,\n",
    "                                               'StoneBr':9,\n",
    "                                               'NoRidge':10, 'NridgHt':10})\n",
    "    \n",
    "    full[\"oCondition1\"] = full.Condition1.map({'Artery':1,\n",
    "                                           'Feedr':2, 'RRAe':2,\n",
    "                                           'Norm':3, 'RRAn':3,\n",
    "                                           'PosN':4, 'RRNe':4,\n",
    "                                           'PosA':5 ,'RRNn':5})\n",
    "    \n",
    "    full[\"oBldgType\"] = full.BldgType.map({'2fmCon':1, 'Duplex':1, 'Twnhs':1, '1Fam':2, 'TwnhsE':2})\n",
    "    \n",
    "    full[\"oHouseStyle\"] = full.HouseStyle.map({'1.5Unf':1, \n",
    "                                           '1.5Fin':2, '2.5Unf':2, 'SFoyer':2, \n",
    "                                           '1Story':3, 'SLvl':3,\n",
    "                                           '2Story':4, '2.5Fin':4})\n",
    "    \n",
    "    full[\"oExterior1st\"] = full.Exterior1st.map({'BrkComm':1,\n",
    "                                             'AsphShn':2, 'CBlock':2, 'AsbShng':2,\n",
    "                                             'WdShing':3, 'Wd Sdng':3, 'MetalSd':3, 'Stucco':3, 'HdBoard':3,\n",
    "                                             'BrkFace':4, 'Plywood':4,\n",
    "                                             'VinylSd':5,\n",
    "                                             'CemntBd':6,\n",
    "                                             'Stone':7, 'ImStucc':7})\n",
    "    \n",
    "    full[\"oMasVnrType\"] = full.MasVnrType.map({'BrkCmn':1, 'None':1, 'BrkFace':2, 'Stone':3})\n",
    "    \n",
    "    full[\"oExterQual\"] = full.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n",
    "    \n",
    "    full[\"oFoundation\"] = full.Foundation.map({'Slab':1, \n",
    "                                           'BrkTil':2, 'CBlock':2, 'Stone':2,\n",
    "                                           'Wood':3, 'PConc':4})\n",
    "    \n",
    "    full[\"oBsmtQual\"] = full.BsmtQual.map({'Fa':2, 'None':1, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "    \n",
    "    full[\"oBsmtExposure\"] = full.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n",
    "    \n",
    "    full[\"oHeating\"] = full.Heating.map({'Floor':1, 'Grav':1, 'Wall':2, 'OthW':3, 'GasW':4, 'GasA':5})\n",
    "    \n",
    "    full[\"oHeatingQC\"] = full.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "    \n",
    "    full[\"oKitchenQual\"] = full.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n",
    "    \n",
    "    full[\"oFunctional\"] = full.Functional.map({'Maj2':1, 'Maj1':2, 'Min1':2, 'Min2':2, 'Mod':2, 'Sev':2, 'Typ':3})\n",
    "    \n",
    "    full[\"oFireplaceQu\"] = full.FireplaceQu.map({'None':1, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "    \n",
    "    full[\"oGarageType\"] = full.GarageType.map({'CarPort':1, 'None':1,\n",
    "                                           'Detchd':2,\n",
    "                                           '2Types':3, 'Basment':3,\n",
    "                                           'Attchd':4, 'BuiltIn':5})\n",
    "    \n",
    "    full[\"oGarageFinish\"] = full.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n",
    "    \n",
    "    full[\"oPavedDrive\"] = full.PavedDrive.map({'N':1, 'P':2, 'Y':3})\n",
    "    \n",
    "    full[\"oSaleType\"] = full.SaleType.map({'COD':1, 'ConLD':1, 'ConLI':1, 'ConLw':1, 'Oth':1, 'WD':1,\n",
    "                                       'CWD':2, 'Con':3, 'New':3})\n",
    "    \n",
    "    full[\"oSaleCondition\"] = full.SaleCondition.map({'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4})            \n",
    "                 \n",
    "    return \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去除两行元素\n",
    "full.drop(\"LotAreaCut\",axis=1,inplace=True)\n",
    "full.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Next we can build a pipeline. It's convenient to experiment different feature combinations once you've got a pipeline.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Label Encoding three \"Year\" features.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#标签编码\n",
    "class labelenc(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        lab=LabelEncoder()\n",
    "        X[\"YearBuilt\"] = lab.fit_transform(X[\"YearBuilt\"])\n",
    "        X[\"YearRemodAdd\"] = lab.fit_transform(X[\"YearRemodAdd\"])\n",
    "        X[\"GarageYrBlt\"] = lab.fit_transform(X[\"GarageYrBlt\"])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Apply log1p to the skewed features, then get_dummies.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#独热编码\n",
    "class skew_dummies(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,skew=0.5):\n",
    "        self.skew = skew\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X_numeric=X.select_dtypes(exclude=[\"object\"])\n",
    "        skewness = X_numeric.apply(lambda x: skew(x))\n",
    "        skewness_features = skewness[abs(skewness) >= self.skew].index\n",
    "        X[skewness_features] = np.log1p(X[skewness_features])\n",
    "        X = pd.get_dummies(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "pipe = Pipeline([\n",
    "    ('labenc', labelenc()),\n",
    "    ('skew_dummies', skew_dummies(skew=1)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the original data for later use\n",
    "full2 = full.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_pipe = pipe.fit_transform(full2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_pipe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_pipe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __use robustscaler since maybe there are other outliers.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train=train.shape[0]\n",
    "\n",
    "X = data_pipe[:n_train]\n",
    "test_X = data_pipe[n_train:]\n",
    "y= train.SalePrice\n",
    "\n",
    "X_scaled = scaler.fit(X).transform(X)\n",
    "y_log = np.log(train.SalePrice)\n",
    "test_X_scaled = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __I have to confess, the feature engineering above is not enough, so we need more.__   \n",
    "+ __Combining different features is usually a good way, but we have no idea what features should we choose. Luckily there are some models that can provide feature selection, here I use Lasso, but you are free to choose Ridge, RandomForest or GradientBoostingTree.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#使用Lasso模型来特征筛选\n",
    "\n",
    "lasso=Lasso(alpha=0.001)\n",
    "lasso.fit(X_scaled,y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FI_lasso = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=data_pipe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FI_lasso.sort_values(\"Feature Importance\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FI_lasso[FI_lasso[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Based on the \"Feature Importance\" plot and other try-and-error, I decided to add some features to the pipeline.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 特征组合，生成新的特征\n",
    "class add_feature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,additional=1):\n",
    "        self.additional = additional\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        if self.additional==1:\n",
    "            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n",
    "            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n",
    "            \n",
    "        else:\n",
    "            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n",
    "            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n",
    "            \n",
    "            X[\"+_TotalHouse_OverallQual\"] = X[\"TotalHouse\"] * X[\"OverallQual\"]\n",
    "            X[\"+_GrLivArea_OverallQual\"] = X[\"GrLivArea\"] * X[\"OverallQual\"]\n",
    "            X[\"+_oMSZoning_TotalHouse\"] = X[\"oMSZoning\"] * X[\"TotalHouse\"]\n",
    "            X[\"+_oMSZoning_OverallQual\"] = X[\"oMSZoning\"] + X[\"OverallQual\"]\n",
    "            X[\"+_oMSZoning_YearBuilt\"] = X[\"oMSZoning\"] + X[\"YearBuilt\"]\n",
    "            X[\"+_oNeighborhood_TotalHouse\"] = X[\"oNeighborhood\"] * X[\"TotalHouse\"]\n",
    "            X[\"+_oNeighborhood_OverallQual\"] = X[\"oNeighborhood\"] + X[\"OverallQual\"]\n",
    "            X[\"+_oNeighborhood_YearBuilt\"] = X[\"oNeighborhood\"] + X[\"YearBuilt\"]\n",
    "            X[\"+_BsmtFinSF1_OverallQual\"] = X[\"BsmtFinSF1\"] * X[\"OverallQual\"]\n",
    "            \n",
    "            X[\"-_oFunctional_TotalHouse\"] = X[\"oFunctional\"] * X[\"TotalHouse\"]\n",
    "            X[\"-_oFunctional_OverallQual\"] = X[\"oFunctional\"] + X[\"OverallQual\"]\n",
    "            X[\"-_LotArea_OverallQual\"] = X[\"LotArea\"] * X[\"OverallQual\"]\n",
    "            X[\"-_TotalHouse_LotArea\"] = X[\"TotalHouse\"] + X[\"LotArea\"]\n",
    "            X[\"-_oCondition1_TotalHouse\"] = X[\"oCondition1\"] * X[\"TotalHouse\"]\n",
    "            X[\"-_oCondition1_OverallQual\"] = X[\"oCondition1\"] + X[\"OverallQual\"]\n",
    "            \n",
    "           \n",
    "            X[\"Bsmt\"] = X[\"BsmtFinSF1\"] + X[\"BsmtFinSF2\"] + X[\"BsmtUnfSF\"]\n",
    "            X[\"Rooms\"] = X[\"FullBath\"]+X[\"TotRmsAbvGrd\"]\n",
    "            X[\"PorchArea\"] = X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n",
    "            X[\"TotalPlace\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"] + X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n",
    "\n",
    "    \n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __By using a pipeline, you can quickily experiment different feature combinations.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('labenc', labelenc()),\n",
    "    ('add_feature', add_feature(additional=2)),\n",
    "    ('skew_dummies', skew_dummies(skew=1)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Im my case, doing PCA is very important. It lets me gain a relatively big boost on leaderboard. At first I don't believe PCA can help me, but \n",
    "in retrospect, maybe the reason is that the features I built are highly correlated, and it leads to multicollinearity. PCA can decorrelate these features.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __So I'll use approximately the same dimension in PCA as  in the original data. Since the aim here is not deminsion reduction.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_pipe = pipe.fit_transform(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_pipe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将数据集重新拆分成训练集和测试集\n",
    "n_train=train.shape[0]\n",
    "X = full_pipe[:n_train]\n",
    "test_X = full_pipe[n_train:]\n",
    "y= train.SalePrice\n",
    "\n",
    "X_scaled = scaler.fit(X).transform(X)\n",
    "y_log = np.log(train.SalePrice)\n",
    "test_X_scaled = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=410)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scaled=pca.fit_transform(X_scaled)\n",
    "test_X_scaled = pca.transform(test_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_scaled.shape, test_X_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义验证函数\n",
    "# define cross validation strategy\n",
    "def rmse_cv(model,X,y):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __We choose 13 models and use 5-folds cross-calidation to evaluate these models.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models include:\n",
    "\n",
    "+ LinearRegression\n",
    "+ Ridge\n",
    "+ Lasso\n",
    "+ Random Forrest\n",
    "+ Gradient Boosting Tree\n",
    "+ Support Vector Regression\n",
    "+ Linear Support Vector Regression\n",
    "+ ElasticNet\n",
    "+ Stochastic Gradient Descent\n",
    "+ BayesianRidge\n",
    "+ KernelRidge\n",
    "+ ExtraTreesRegressor\n",
    "+ XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [LinearRegression(),Ridge(),Lasso(alpha=0.01,max_iter=10000),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n",
    "          ElasticNet(alpha=0.001,max_iter=10000),SGDRegressor(),BayesianRidge(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n",
    "          ExtraTreesRegressor(),XGBRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = [\"LR\", \"Ridge\", \"Lasso\", \"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\"]\n",
    "for name, model in zip(names, models):\n",
    "    score = rmse_cv(model, X_scaled, y_log)\n",
    "    print(\"{}: {:.6f}, {:.4f}\".format(name,score.mean(),score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Next we do some hyperparameters tuning. First define a gridsearch method.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class grid():\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "    \n",
    "    def grid_get(self,X,y,param_grid):\n",
    "        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n",
    "        grid_search.fit(X,y)\n",
    "        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n",
    "        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n",
    "        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid(Lasso()).grid_get(X_scaled,y_log,{'alpha': [0.0004,0.0005,0.0007,0.0006,0.0009,0.0008],'max_iter':[10000]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid(Ridge()).grid_get(X_scaled,y_log,{'alpha':[35,40,45,50,55,60,65,70,80,90]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid(SVR()).grid_get(X_scaled,y_log,{'C':[11,12,13,14,15],'kernel':[\"rbf\"],\"gamma\":[0.0003,0.0004],\"epsilon\":[0.008,0.009]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid={'alpha':[0.2,0.3,0.4,0.5], 'kernel':[\"polynomial\"], 'degree':[3],'coef0':[0.8,1,1.2]}\n",
    "grid(KernelRidge()).grid_get(X_scaled,y_log,param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid(ElasticNet()).grid_get(X_scaled,y_log,{'alpha':[0.0005,0.0008,0.004,0.005],'l1_ratio':[0.08,0.1,0.3,0.5,0.7],'max_iter':[10000]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Average base models according to their weights.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AverageWeight(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,mod,weight):\n",
    "        self.mod = mod\n",
    "        self.weight = weight\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.models_ = [clone(x) for x in self.mod]\n",
    "        for model in self.models_:\n",
    "            model.fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        w = list()\n",
    "        pred = np.array([model.predict(X) for model in self.models_])\n",
    "        # for every data point, single model prediction times weight, then add them together\n",
    "        for data in range(pred.shape[1]):\n",
    "            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n",
    "            w.append(np.sum(single))\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#最终选择出的模型\n",
    "lasso = Lasso(alpha=0.0005,max_iter=10000)\n",
    "ridge = Ridge(alpha=60)\n",
    "svr = SVR(gamma= 0.0004,kernel='rbf',C=13,epsilon=0.009)\n",
    "ker = KernelRidge(alpha=0.2 ,kernel='polynomial',degree=3 , coef0=0.8)\n",
    "ela = ElasticNet(alpha=0.005,l1_ratio=0.08,max_iter=10000)\n",
    "bay = BayesianRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign weights based on their gridsearch score\n",
    "w1 = 0.02\n",
    "w2 = 0.2\n",
    "w3 = 0.25\n",
    "w4 = 0.3\n",
    "w5 = 0.03\n",
    "w6 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_avg = AverageWeight(mod = [lasso,ridge,svr,ker,ela,bay],weight=[w1,w2,w3,w4,w5,w6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse_cv(weight_avg,X_scaled,y_log),  rmse_cv(weight_avg,X_scaled,y_log).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __But if we average only two best models, we gain better cross-validation score.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_avg = AverageWeight(mod = [svr,ker],weight=[0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse_cv(weight_avg,X_scaled,y_log),  rmse_cv(weight_avg,X_scaled,y_log).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Aside from normal stacking, I also add the \"get_oof\" method, because later I'll combine features generated from stacking and original features.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self,mod,meta_model):\n",
    "        self.mod = mod\n",
    "        self.meta_model = meta_model\n",
    "        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.saved_model = [list() for i in self.mod]\n",
    "        oof_train = np.zeros((X.shape[0], len(self.mod)))\n",
    "        \n",
    "        for i,model in enumerate(self.mod):\n",
    "            for train_index, val_index in self.kf.split(X,y):\n",
    "                renew_model = clone(model)\n",
    "                renew_model.fit(X[train_index], y[train_index])\n",
    "                self.saved_model[i].append(renew_model)\n",
    "                oof_train[val_index,i] = renew_model.predict(X[val_index])\n",
    "        \n",
    "        self.meta_model.fit(oof_train,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n",
    "                                      for single_model in self.saved_model]) \n",
    "        return self.meta_model.predict(whole_test)\n",
    "    \n",
    "    def get_oof(self,X,y,test_X):\n",
    "        oof = np.zeros((X.shape[0],len(self.mod)))\n",
    "        test_single = np.zeros((test_X.shape[0],5))\n",
    "        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n",
    "        for i,model in enumerate(self.mod):\n",
    "            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n",
    "                clone_model = clone(model)\n",
    "                clone_model.fit(X[train_index],y[train_index])\n",
    "                oof[val_index,i] = clone_model.predict(X[val_index])\n",
    "                test_single[:,j] = clone_model.predict(test_X)\n",
    "            test_mean[:,i] = test_single.mean(axis=1)\n",
    "        return oof, test_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Let's first try it out ! It's a bit slow to run this method, since the process is quite compliated. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# must do imputer first, otherwise stacking won't work, and i don't know why.\n",
    "a = Imputer().fit_transform(X_scaled)\n",
    "b = Imputer().fit_transform(y_log.values.reshape(-1,1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rmse_cv(stack_model,a,b))\n",
    "print(rmse_cv(stack_model,a,b).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Next we extract the features generated from stacking, then combine them with original features.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_stack, X_test_stack = stack_model.get_oof(a,b,test_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_stack.shape, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_add = np.hstack((a,X_train_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_add = np.hstack((test_X_scaled,X_test_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_add.shape, X_test_add.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rmse_cv(stack_model,X_train_add,b))\n",
    "print(rmse_cv(stack_model,X_train_add,b).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __You can even do parameter tuning for your meta model after you get \"X_train_stack\", or do it after combining with the original features. but that's a lot of work too !__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the final model I use\n",
    "stack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stack_model.fit(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = np.exp(stack_model.predict(test_X_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result=pd.DataFrame({'Id':test.Id, 'SalePrice':pred})\n",
    "result.to_csv(\"../out/submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
