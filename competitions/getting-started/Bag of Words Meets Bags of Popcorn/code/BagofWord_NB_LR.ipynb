{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考地址\n",
    "[https://github.com/apachecn/kaggle/tree/dev/competitions/getting-started/word2vec-nlp-tutorial](https://github.com/apachecn/kaggle/tree/dev/competitions/getting-started/word2vec-nlp-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载依赖包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_dir = \"../data\"\n",
    "# 载入数据集\n",
    "train = pd.read_csv('%s/%s' % (root_dir, 'labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv('%s/%s' % (root_dir, 'testData.tsv'), header=0, delimiter=\"\\t\", quoting=3)\n",
    "test[\"id\"] = test[\"id\"].apply(lambda x: eval(x))\n",
    "print(train.shape)\n",
    "print(train.columns.values)\n",
    "print(train.head(3))\n",
    "print(test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 去除评论中的HTML标签\n",
    "print('\\n处理前: \\n', train['review'][0])\n",
    "\n",
    "example1 = BeautifulSoup(train['review'][0], \"html.parser\")\n",
    "\n",
    "import re\n",
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub('[^a-zA-Z]',  # 搜寻的pattern\n",
    "                      ' ',           # 用来替代的pattern(空格)\n",
    "                      example1.get_text())  # 待搜索的text \n",
    "\n",
    "print(letters_only)\n",
    "lower_case = letters_only.lower()  # Convert to lower case\n",
    "words = lower_case.split()  # Split into word\n",
    "\n",
    "print('\\n处理后: \\n', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review):\n",
    "    '''\n",
    "    把IMDB的评论转成词序列\n",
    "    参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613\n",
    "    '''\n",
    "    # 去掉HTML标签，拿到内容\n",
    "    review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    # 用正则表达式取出符合规范的部分\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    # 小写化所有的词，并转成词list\n",
    "    words = review_text.lower().split()\n",
    "    # 返回words\n",
    "    return words\n",
    "\n",
    "\n",
    "# 预处理数据\n",
    "label = train['sentiment']\n",
    "train_data = []\n",
    "for i in range(len(train['review'])):\n",
    "    train_data.append(' '.join(review_to_wordlist(train['review'][i])))\n",
    "test_data = []\n",
    "for i in range(len(test['review'])):\n",
    "    test_data.append(' '.join(review_to_wordlist(test['review'][i])))\n",
    "\n",
    "# 预览数据\n",
    "print(train_data[0], '\\n')\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "# 参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613\n",
    "\n",
    "\"\"\"\n",
    "min_df: 最小支持度为2（词汇出现的最小次数）\n",
    "max_features: 默认为None，可设为int，对所有关键词的term frequency进行降序排序，只取前max_features个作为关键词集\n",
    "strip_accents: 将使用ascii或unicode编码在预处理步骤去除raw document中的重音符号\n",
    "analyzer: 设置返回类型\n",
    "token_pattern: 表示token的正则表达式，需要设置analyzer == 'word'，默认的正则表达式选择2个及以上的字母或数字作为token，标点符号默认当作token分隔符，而不会被当作token\n",
    "ngram_range: 词组切分的长度范围\n",
    "use_idf: 启用逆文档频率重新加权\n",
    "use_idf：默认为True，权值是tf*idf，如果设为False，将不使用idf，就是只使用tf，相当于CountVectorizer了。\n",
    "smooth_idf: idf平滑参数，默认为True，idf=ln((文档总数+1)/(包含该词的文档数+1))+1，如果设为False，idf=ln(文档总数/包含该词的文档数)+1\n",
    "sublinear_tf: 默认为False，如果设为True，则替换tf为1 + log(tf)\n",
    "stop_words: 设置停用词，设为english将使用内置的英语停用词，设为一个list可自定义停用词，设为None不使用停用词，设为None且max_df∈[0.7, 1.0)将自动根据当前的语料库建立停用词表\n",
    "\"\"\"\n",
    "tfidf = TFIDF(min_df=2,\n",
    "           max_features=None,\n",
    "           strip_accents='unicode',\n",
    "           analyzer='word',\n",
    "           token_pattern=r'\\w{1,}',\n",
    "           ngram_range=(1, 3),  # 二元文法模型\n",
    "           use_idf=1,\n",
    "           smooth_idf=1,\n",
    "           sublinear_tf=1,\n",
    "           stop_words = 'english') # 去掉英文停用词\n",
    "\n",
    "# 合并训练和测试集以便进行TFIDF向量化操作\n",
    "data_all = train_data + test_data\n",
    "len_train = len(train_data)\n",
    "\n",
    "tfidf.fit(data_all)\n",
    "data_all = tfidf.transform(data_all)\n",
    "# 恢复成训练集和测试集部分\n",
    "train_x = data_all[:len_train]\n",
    "test_x = data_all[len_train:]\n",
    "print('TF-IDF处理结束.')\n",
    "\n",
    "print(\"train: \\n\", np.shape(train_x[0]))\n",
    "print(\"test: \\n\", np.shape(test_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 朴素贝叶斯训练\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "\n",
    "model_NB = MNB() # (alpha=1.0, class_prior=None, fit_prior=True)\n",
    "# 为了在预测的时候使用\n",
    "model_NB.fit(train_x, label)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"多项式贝叶斯分类器10折交叉验证得分:  \\n\", cross_val_score(model_NB, train_x, label, cv=10, scoring='roc_auc'))\n",
    "print(\"\\n多项式贝叶斯分类器10折交叉验证得分: \", np.mean(cross_val_score(model_NB, train_x, label, cv=10, scoring='roc_auc')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predicted = np.array(model_NB.predict(test_x))\n",
    "print('保存结果...')\n",
    "\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(10))\n",
    "submission_df.to_csv('../out/submission_br.csv',columns = ['id','sentiment'], index = False)\n",
    "\n",
    "# nb_output = pd.DataFrame(data=test_predicted, columns=['sentiment'])\n",
    "# nb_output['id'] = test['id']\n",
    "# nb_output = nb_output[['id', 'sentiment']]\n",
    "# nb_output.to_csv('nb_output.csv', index=False)\n",
    "print('结束.')\n",
    "\n",
    "'''\n",
    "1.提交最终的结果到kaggle，AUC为：0.85728，排名300左右，50%的水平\n",
    "2. ngram_range = 3, 三元文法，AUC为0.85924\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 设定grid search的参数\n",
    "grid_values = {'C': [1, 15, 30, 50]}  \n",
    "# grid_values = {'C': [30]}\n",
    "# 设定打分为roc_auc\n",
    "\"\"\"\n",
    "penalty: l1 or l2, 用于指定惩罚中使用的标准。\n",
    "\"\"\"\n",
    "model_LR = GridSearchCV(LR(penalty='l2', dual=True, random_state=0), grid_values, scoring='roc_auc', cv=20)\n",
    "model_LR.fit(train_x, label)\n",
    "# 20折交叉验证\n",
    "# GridSearchCV(cv=20, \n",
    "#         estimator=LR(C=1.0, \n",
    "#             class_weight=None, \n",
    "#             dual=True, \n",
    "#             fit_intercept=True, \n",
    "#             intercept_scaling=1, \n",
    "#             penalty='l2', \n",
    "#             random_state=0, \n",
    "#             tol=0.0001),\n",
    "#         fit_params={}, \n",
    "#         iid=True,\n",
    "#         n_jobs=1,\n",
    "#         param_grid={'C': [30]}, \n",
    "#         pre_dispatch='2*n_jobs',\n",
    "#         refit=True,\n",
    "#         scoring='roc_auc', \n",
    "#         verbose=0)\n",
    "\n",
    "# 输出结果\n",
    "# print(model_LR.grid_scores_, '\\n', model_LR.best_params_, model_LR.best_params_)\n",
    "print(model_LR.cv_results_, '\\n', model_LR.best_params_, model_LR.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_LR = LR(penalty='l2', dual=True, random_state=0)\n",
    "model_LR.fit(train_x, label)\n",
    "\n",
    "test_predicted = np.array(model_LR.predict(test_x))\n",
    "print('保存结果...')\n",
    "\n",
    "test[\"sentiment\"] = test_predicted\n",
    "test = test[['id','sentiment']]\n",
    "test.to_csv('../out/submission_lr.csv',index=False)\n",
    "\n",
    "\n",
    "'''\n",
    "1. 提交最终的结果到kaggle，AUC为：0.88956，排名260左右，比之前贝叶斯模型有所提高\n",
    "2. 三元文法，AUC为0.89076\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3.Word2vec向量\n",
    "---\n",
    "\n",
    "神经网络语言模型L = SUM[log(p(w|contect(w))]，即在w的上下文下计算当前词w的概率，由公式可以看到，我们的核心是计算p(w|contect(w)， Word2vec给出了构造这个概率的一个方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # review = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    # print(words)\n",
    "    return(words)\n",
    "\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    '''\n",
    "    1. 将评论文章，按照句子段落来切分(所以会比文章的数量多很多)\n",
    "    2. 返回句子列表，每个句子由一堆词组成\n",
    "    '''\n",
    "    review = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    # raw_sentences 句子段落集合\n",
    "    raw_sentences = tokenizer.tokenize(review)\n",
    "    # print(raw_sentences)\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            # 获取句子中的词列表\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "sentences = []\n",
    "for i, review in enumerate(train[\"review\"]):\n",
    "    # print(i, review)\n",
    "    sentences += review_to_sentences(review, tokenizer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(train[\"review\"]))\n",
    "print(np.shape(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unlabeled_train = pd.read_csv(\"%s/%s\" % (root_dir, \"unlabeledTrainData.tsv\"), header=0, delimiter=\"\\t\", quoting=3 )\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "print('预处理 unlabeled_train data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(train_data))\n",
    "print(np.shape(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "# 模型参数\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 训练模型\n",
    "print(\"训练模型中...\")\n",
    "model = Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count=min_word_count, \\\n",
    "            window=context, sample=downsampling)\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('保存模型...')\n",
    "model.init_sims(replace=True)\n",
    "model_name = \"%s/%s\" % (root_dir, \"300features_40minwords_10context\")\n",
    "model.save(model_name)\n",
    "print('保存结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# help(model.wv.most_similar)\n",
    "model.wv.most_similar(\"man\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"queen\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"awful\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    '''\n",
    "    对段落中的所有词向量进行取平均操作\n",
    "    '''\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "\n",
    "    # Index2word包含了词表中的所有词，为了检索速度，保存到set中\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    # 取平均\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    '''\n",
    "    给定一个文本列表，每个文本由一个词列表组成，返回每个文本的词向量平均值\n",
    "    '''\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "        if counter % 5000 == 0:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time trainDataVecs = getAvgFeatureVecs(train_data, model, num_features)\n",
    "print(np.shape(trainDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time testDataVecs = getAvgFeatureVecs(test_data, model, num_features)\n",
    "print(np.shape(testDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB as GNB\n",
    "\n",
    "model_GNB = GNB()\n",
    "model_GNB.fit(trainDataVecs, label)\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"高斯贝叶斯分类器10折交叉验证得分: \", np.mean(cross_val_score(model_GNB, trainDataVecs, label, cv=10, scoring='roc_auc')))\n",
    "\n",
    "print('保存结果...')\n",
    "result = model_GNB.predict( testDataVecs )\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': result})\n",
    "print(submission_df.head(10))\n",
    "submission_df.to_csv('/Users/jiangzl/Desktop/gnb_word2vec.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')\n",
    "\n",
    "\"\"\"\n",
    "从验证结果来看，没有超过基于TF-IDF多项式贝叶斯模型\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier( n_estimators = 100, n_jobs=2)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "%time forest = forest.fit( trainDataVecs, label )\n",
    "print(\"随机森林分类器10折交叉验证得分: \", np.mean(cross_val_score(forest, trainDataVecs, label, cv=10, scoring='roc_auc')))\n",
    "\n",
    "# 测试集\n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': result})\n",
    "print(submission_df.head(10))\n",
    "submission_df.to_csv('/Users/jiangzl/Desktop/rf_word2vec.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')\n",
    "\n",
    "\"\"\"\n",
    "改用随机森林之后，效果有提升，但是依然没有超过基于TF-IDF多项式贝叶斯模型\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载训练好的词向量\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = Word2Vec.load_word2vec_format(\"vector.txt\", binary=False)  # C text format\n",
    "# model = Word2Vec.load_word2vec_format(\"vector.bin\", binary=True)  # C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载 google 的词向量，查看单词之间关系\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "model = Word2Vec.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 测试预测效果\n",
    "\n",
    "print(model.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=5))\n",
    "print(model.most_similar(positive=[\"biggest\", \"small\"], negative=[\"big\"], topn=5))\n",
    "print(model.most_similar(positive=[\"ate\", \"speak\"], negative=[\"eat\"], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"food_words.txt\", \"r\") as infile:\n",
    "    food_words = infile.readlines()\n",
    "    \n",
    "with open(\"sports_words.txt\", \"r\") as infile:\n",
    "    food_words = infile.readlines()\n",
    "    \n",
    "with open(\"weather_words.txt\", \"r\") as infile:\n",
    "    food_words = infile.readlines()\n",
    "    \n",
    "def getWordVecs(words):\n",
    "    vec = []\n",
    "    for word in words:\n",
    "        word = word.replace(\"\\n\", \"\")\n",
    "        try:\n",
    "            vecs.append(model[word].reshape((1, 300)))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # numpy提供了numpy.concatenate((a1,a2,...), axis=0)函数。能够一次完成多个数组的拼接\n",
    "    \"\"\"\n",
    "    >>> a=np.array([1,2,3])\n",
    "    >>> b=np.array([11,22,33])\n",
    "    >>> c=np.array([44,55,66])\n",
    "    >>> np.concatenate((a,b,c),axis=0)  # 默认情况下，axis=0可以不写\n",
    "    array([ 1,  2,  3, 11, 22, 33, 44, 55, 66]) #对于一维数组拼接，axis的值不影响最后的结果\n",
    "    \"\"\"\n",
    "    vecs = np.concatenate(vecs)\n",
    "    return np.array(vecs, dtype=\"float\")\n",
    "\n",
    "food_vecs = getWordVecs(food_words)\n",
    "sports_vecs = getWordVecs(sports_words)\n",
    "weather_vecs = getWordVecs(weather_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 利用 TSNE 和 matplotlib 对分类结果进行可视化处理\n",
    "\n",
    "from sklearn.manifold import TSEN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ts = TSEN(2)\n",
    "reduced_vecs = ts.fit_transform(np.concatenate((food_vecs, sports_vecs, weather_vecs)))\n",
    "\n",
    "for i in range(len(reduced_vecs)):\n",
    "    if i < len(food_vecs):\n",
    "        color = \"b\"\n",
    "    elif i >= len(food_vecs) and i <(len(food_vecs)+len(sports_vecs)):\n",
    "        color = \"r\"\n",
    "    else:\n",
    "        color = \"g\"\n",
    "    \n",
    "    plt.plot(reduced_vecs[i, 0], reduced_vecs[i, 1], marker=\"0\", color=color, marksize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 首先，我们导入数据并构建 Word2Vec 模型：\n",
    "\n",
    "from sklearn.cross_validation import train_ _test_ _split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "with open('twitter.data/pos_ tweets.txt', 'r') as infile:\n",
    "    pos_tweets= infile.readlines()\n",
    "\n",
    "with open(' twitter_ data/neg_ tweets.txt', 'r') as infile:\n",
    "    neg_ _tweets = infile.readlines()\n",
    "\n",
    "# use 1for positive sentiment,0 for negative\n",
    "Y= np.concatenate((np.ones( len (pos_tweets )) ，np.zeros(len(neg_tweets))))\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(np.concatenate((pos_tweets, neg_tweets)), y, test_size=0.2)\n",
    "# Do some very minor text preprocessing\n",
    "\n",
    "def cleanText(corpus):\n",
    "    corpus= [z.lower( ).replace(' \\n' , '').split() for z in corpus]\n",
    "    return corpus\n",
    "\n",
    "x_ train= cleanText(x_ train)\n",
    "x_ test= cleanText (x_ _test)\n",
    "\n",
    "n _dim= 300\n",
    "#Initialize model and build vocab\n",
    "imdb_w2v= Word2Vec(size=n dim, min_count=10)\n",
    "imdb_w2v.build_vocab(x_ _train)\n",
    "#Train the model over train_ _reviews (this may take several minutes)\n",
    "imdb_w2v.train( x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 接下来，为了利用下面的函数获得推文中所有词向量的平均值，我们必须构建作为输入文本的词向量。\n",
    "\n",
    "def buildWordVector(text, size):\n",
    "    vec = np.zeros(size).reshape((1，size))\n",
    "    count= 0.\n",
    "\n",
    "    for word in text :\n",
    "        try:\n",
    "            vec += imdb_w2v[word].reshape( (1，size) )\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec 1'= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 调整数据集的量纲是数据标准化处理的一部分，我们通常将数据集转化成服从均值为零的高斯分布，这说明数值大于均值表示乐观，反之则表示悲观。为了使模型更有效，许多机器学习模型需要预先处理数据集的量纲，特别是文本分类器这类具有许多变量的模型。\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "train_vecs = np.concatenate([buildWordVector(z ，n_dim) for z in x_train])\n",
    "train_vecs= scale(train_vecs)\n",
    "\n",
    "# Train word2vec on test tweets\n",
    "imdb_w2v.train(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 最后我们需要建立测试集向量并对其标准化处理：\n",
    "\n",
    "#Build test tweet vectors then scale\n",
    "test_vecs = np.concatenate( [buildWordVector( Z，n _dim) for z in x _test ])\n",
    "test_vecs = scale(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "接下来我们想要通过计算测试集的预测精度和 ROC 曲线来验证分类器的有效性。 ROC 曲线衡量当模型参数调整的时候，其真阳性率和假阳性率的变化情况。在我们的案例中，我们调整的是分类器模型截断阈值的概率。一般来说，ROC 曲线下的面积（AUC）越大，该模型的表现越好。你可以在这里找到更多关于 ROC 曲线的资料\n",
    "\n",
    "（https://en.wikipedia.org/wiki/Receiver_operating_characteristic）\n",
    "\n",
    "在这个案例中我们使用罗吉斯回归的随机梯度下降法作为分类器算法。\n",
    "\"\"\"\n",
    "\n",
    "#Use classification algorithm (i.e.Stochastic Logistic Regression) on training set, then assess model performance on test set\n",
    "\n",
    "from sklearn.linear model import SGDClassifier\n",
    "lr = SGDClassifier(loss='log' ，penalty='11' )\n",
    "lr.fit(train_vecs, y_train)\n",
    "print' Test Accuracy: %.2f' % r.score(test vecs, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 随后我们利用 matplotlib 和 metric 库来构建 ROC 曲线\n",
    "\n",
    "#Crea t e ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_probas = lr.predict_proba(test_vecs)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_probas )\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr,tpr,label='area = %.2f' % roc_ auc)\n",
    "plt.plot([0,1]，[0，1],'k--')\n",
    "plt. xlim( [0. 0 ，1. 0 ])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
